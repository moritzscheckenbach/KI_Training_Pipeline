{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dce0dc2",
   "metadata": {},
   "source": [
    "## Example of a Neural Network Pipeline with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7ccc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f66dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Vision Transformer (ViT) implementation\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, image_size=28, patch_size=7, num_classes=10, dim=64, depth=4, heads=4, mlp_dim=128):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = patch_size * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.patch_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5).reshape(B, -1, C * self.patch_size * self.patch_size)\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555ea017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Swin Transformer implementation\n",
    "class SimpleSwinTransformer(nn.Module):\n",
    "    def __init__(self, image_size=28, patch_size=7, num_classes=10, dim=64, depth=4, heads=4, mlp_dim=128):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = patch_size * patch_size\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.patch_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5).reshape(B, -1, C * self.patch_size * self.patch_size)\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86772dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ 'runs' directory already exists. Using existing logs directory...\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# check of run path exists and create if not\n",
    "if os.path.exists('runs'):\n",
    "    print(\"✅ 'runs' directory already exists. Using existing logs directory...\")\n",
    "else:\n",
    "    print(\"❗ 'runs' directory does not exist. Creating new logs directory...\")\n",
    "    os.makedirs('runs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19dda68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charon\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "#model = SimpleViT() # or SimpleSwinTransformer()\n",
    "model = SimpleSwinTransformer() # or SimpleViT()\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fab2c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorBoard writer created successfully!\n",
      "Logging to: runs\\Aug05_17-00-45_Pluto\n",
      "\n",
      "🏋️ Starting training...\n",
      "Batch 0, Loss: 2.5291\n",
      "Batch 0, Loss: 2.5291\n",
      "Batch 10, Loss: 2.3125\n",
      "Batch 10, Loss: 2.3125\n",
      "Batch 20, Loss: 2.3138\n",
      "Batch 20, Loss: 2.3138\n",
      "Batch 30, Loss: 2.2914\n",
      "Batch 30, Loss: 2.2914\n",
      "Batch 40, Loss: 2.3098\n",
      "Batch 40, Loss: 2.3098\n",
      "Batch 50, Loss: 2.3000\n",
      "Batch 50, Loss: 2.3000\n",
      "Batch 60, Loss: 2.3218\n",
      "Batch 60, Loss: 2.3218\n",
      "Batch 70, Loss: 2.2674\n",
      "Batch 70, Loss: 2.2674\n",
      "Batch 80, Loss: 2.2345\n",
      "Batch 80, Loss: 2.2345\n",
      "Batch 90, Loss: 2.0386\n",
      "Batch 90, Loss: 2.0386\n",
      "Batch 100, Loss: 1.8554\n",
      "\n",
      "✅ Training complete!\n",
      "📊 TensorBoard logs saved to: runs\\Aug05_17-00-45_Pluto\n",
      "🌐 Now run the TensorBoard launcher below to visualize the results!\n",
      "Batch 100, Loss: 1.8554\n",
      "\n",
      "✅ Training complete!\n",
      "📊 TensorBoard logs saved to: runs\\Aug05_17-00-45_Pluto\n",
      "🌐 Now run the TensorBoard launcher below to visualize the results!\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard writer - use default runs directory\n",
    "try:\n",
    "    writer = SummaryWriter()\n",
    "    print(\"✅ TensorBoard writer created successfully!\")\n",
    "    print(f\"Logging to: {writer.log_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating TensorBoard writer: {e}\")\n",
    "    # Fallback: create manual logs\n",
    "    import tempfile\n",
    "    temp_dir = tempfile.mkdtemp(prefix='tensorboard_')\n",
    "    writer = SummaryWriter(temp_dir)\n",
    "    print(f\"✅ Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Training loop with logging\n",
    "print(\"\\n🏋️ Starting training...\")\n",
    "model.train()\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log every 10 batches\n",
    "    if batch_idx % 10 == 0:\n",
    "        writer.add_scalar('Loss/Train', loss.item(), batch_idx)\n",
    "        print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    if batch_idx >= 100:  # limit for demo\n",
    "        break\n",
    "\n",
    "# Log final metrics\n",
    "writer.add_scalar('Final/TrainingLoss', loss.item(), 0)\n",
    "writer.close()\n",
    "\n",
    "print(f\"\\n✅ Training complete!\")\n",
    "print(f\"📊 TensorBoard logs saved to: {writer.log_dir}\")\n",
    "print(\"🌐 Now run the TensorBoard launcher below to visualize the results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorBoard is available!\n",
      "\n",
      "📋 Next steps:\n",
      "1. Make sure you have run the training loop (cell 6)\n",
      "2. Use one of the TensorBoard launcher cells below\n",
      "3. Open http://localhost:6006 in your browser\n",
      "\n",
      "💡 Note: On Windows, you need to run TensorBoard via Python module, not directly from command line\n"
     ]
    }
   ],
   "source": [
    "# Check if tensorboard is installed, if not install it\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    print(\"✅ TensorBoard is available!\")\n",
    "except ImportError:\n",
    "    print(\"Tensorboard is not installed. Installing now...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorboard'])\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    print(\"✅ TensorBoard installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a2a179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorBoard...\n",
      "Log directory: c:\\Users\\Charon\\Unterlagen\\Uni\\Master\\Faecher\\RKIM_FuE_KI_Pipeline\\Test_Software\\runs\n",
      "TensorBoard will be available at: http://localhost:6006\n",
      "Press Ctrl+C in the terminal to stop TensorBoard\n",
      "\n",
      "If TensorBoard started successfully, you can access it at:\n",
      "http://localhost:6006\n",
      "\n",
      "To stop TensorBoard, restart the kernel or use Ctrl+C in the terminal.\n",
      "\n",
      "If TensorBoard started successfully, you can access it at:\n",
      "http://localhost:6006\n",
      "\n",
      "To stop TensorBoard, restart the kernel or use Ctrl+C in the terminal.\n"
     ]
    }
   ],
   "source": [
    "# Launch TensorBoard from within the notebook\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def launch_tensorboard():\n",
    "    \"\"\"Launch TensorBoard in a separate thread\"\"\"\n",
    "    try:\n",
    "        # Get the current directory\n",
    "        current_dir = os.getcwd()\n",
    "        runs_dir = os.path.join(current_dir, 'runs')\n",
    "        \n",
    "        # Check if runs directory exists\n",
    "        if not os.path.exists(runs_dir):\n",
    "            print(f\"Warning: {runs_dir} directory not found. Make sure you have run the training loop first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Starting TensorBoard...\")\n",
    "        print(f\"Log directory: {runs_dir}\")\n",
    "        print(\"TensorBoard will be available at: http://localhost:6006\")\n",
    "        print(\"Press Ctrl+C in the terminal to stop TensorBoard\")\n",
    "        \n",
    "        # Launch TensorBoard using Python module\n",
    "        cmd = [sys.executable, '-m', 'tensorboard.main', '--logdir', runs_dir, '--port', '6006']\n",
    "        process = subprocess.run(cmd, check=True)\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error starting TensorBoard: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTensorBoard stopped by user\")\n",
    "\n",
    "# Start TensorBoard in a separate thread so it doesn't block the notebook\n",
    "tensorboard_thread = threading.Thread(target=launch_tensorboard, daemon=True)\n",
    "tensorboard_thread.start()\n",
    "\n",
    "# Give it a moment to start\n",
    "time.sleep(2)\n",
    "print(\"\\nIf TensorBoard started successfully, you can access it at:\")\n",
    "print(\"http://localhost:6006\")\n",
    "print(\"\\nTo stop TensorBoard, restart the kernel or use Ctrl+C in the terminal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2b848",
   "metadata": {},
   "source": [
    "## 📊 TensorBoard Anleitung\n",
    "\n",
    "### Was ist TensorBoard?\n",
    "TensorBoard ist ein Visualisierungstool für Machine Learning Experimente. Es hilft dir dabei:\n",
    "- **Trainingsverläufe** zu visualisieren (Loss, Accuracy, etc.)\n",
    "- **Modellarchitekturen** zu betrachten\n",
    "- **Histogramme** von Gewichten und Gradienten zu analysieren\n",
    "- **Bilder und Embeddings** zu visualisieren\n",
    "\n",
    "### 🚀 Wie verwendest du TensorBoard?\n",
    "\n",
    "#### Schritt 1: Training ausführen\n",
    "Stelle sicher, dass du die Trainingszellen ausgeführt hast. Diese erstellen die Log-Dateien im `runs/` Ordner.\n",
    "\n",
    "#### Schritt 2: TensorBoard öffnen\n",
    "- Öffne deinen Browser\n",
    "- Gehe zu: `http://localhost:6006`\n",
    "- Du siehst jetzt deine Trainingsgraphen!\n",
    "\n",
    "### 🔍 Was siehst du in TensorBoard?\n",
    "- **SCALARS Tab**: Hier siehst du den Trainingsloss über die Zeit\n",
    "- **GRAPHS Tab**: Hier kannst du die Modellarchitektur visualisieren\n",
    "- **DISTRIBUTIONS/HISTOGRAMS**: Gewichtsverteilungen (falls geloggt)\n",
    "\n",
    "2. **SCALARS Tab**: \n",
    "   - Du siehst einen Graphen namens \"Loss/Train\"\n",
    "   - Dieser zeigt, wie der Trainingsloss über die 100 Batches sinkt (von ~2.5 auf ~2.0)\n",
    "   - Du siehst auch \"Final/TrainingLoss\" mit dem finalen Loss-Wert\n",
    "\n",
    "3. **Was die Graphen bedeuten**:\n",
    "   - X-Achse: Batch-Nummer (0 bis 100)\n",
    "   - Y-Achse: Loss-Wert\n",
    "   - Der Graph sollte eine fallende Tendenz zeigen → das Modell lernt! 📈\n",
    "\n",
    "   ### 💡 Tipps\n",
    "- TensorBoard aktualisiert sich automatisch, wenn neue Daten hinzugefügt werden\n",
    "- Du kannst mehrere Experimente vergleichen, indem du verschiedene Ordner in `runs/` erstellst\n",
    "- Verwende aussagekräftige Namen für deine Logs: `writer = SummaryWriter('runs/experiment_1')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cf6c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
